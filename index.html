<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="AutoMA: Automated Modular Attention enables Context-Rich Imitation Learning using Foundation Models">
  <meta name="keywords" content="Imitation Learning, Modular Policy">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Diff-Control: A Stateful Diffusion-based Policy for Imitation Learning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/irl_lab.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://diff-control.github.io/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://languageforrobots.com/">
              Modularity through Attention (ModAttn)
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AutoMA: Automated Modular Attention enables Context-Rich Imitation
              Learning using Foundation Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="http://yifanzhou.com/">Yifan Zhou</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.xiao-liu.me/">Xiao Liu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://quanvuong.github.io/">Quan Vuong</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://henibenamor.weebly.com/">Heni Ben Amor</a><sup>1</sup>,
              </span>
              <!-- <span class="author-block">
                <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
              </span> -->
              <!-- <span class="author-block">
                <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
              </span> -->
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup><a
                  href="https://interactive-robotics.engineering.asu.edu/">Interactive Robotics Lab, Arizona State
                  University</span></a>
              <span class="author-block"><sup>2</sup><a href="https://physicalintelligence.company/">Physical
                  Intelligence</span></a>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="./static/videos/Diff-Control.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2011.12948" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="./static/videos/task_sequence.mp4" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/ir-lab/Diff-Control"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://www.dropbox.com/scl/fo/2onj92s7gewettu1rjg3d/h?rlkey=1d4dnhwf3z6s4a51lopgocby5&dl=0"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">

        <img src="./static/videos/teaser-1.png" class="interpolation-image" alt="Interpolate start reference image." />

        <!-- <video id="teaser" autoplay controls muted loop playsinline height="100%">
          <source src="./static/videos/task_sequence.mp4" type="video/mp4">
        </video> -->
        <h2 class="subtitle has-text-centered">

          Given the task "<i>stack the blue block on the red block</i> ", LLMs design hierarchical modules h by
          decomposing the task into several sub-task modules, while VLMs supervises the module. The policy network,
          enriched with this context, can then effectively execute the task.

        </h2>
      </div>
    </div>
  </section>


  <!-- <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/steve.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/chair-tp.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/shiba.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fullbody.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/blueshirt.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/mask.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-coffee">
            <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/coffee.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/toby2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Although imitation learning offers an appealing framework for policy learning, scaling it is challenging
              due to the lack of high-quality expert data. Previous works have addressed this by modifying model
              design
              and increasing state diversity to improve data quality, but these approaches often overlook the rich
              contextual information inherent in the data itself. In this paper, we propose an automated learning
              framework that leverages large language models (LLMs) and vision-language models (VLMs) to provide
              context-rich supervision for Modular Attention based imitation learning policies, denoted as
              <strong>AutoMA</strong>.
              Specifically, LLMs are used for hierarchical modular design, and VLMs provide supervision signals for
              the
              different modules. <strong>AutoMA</strong> thus leverages the capabilities of foundational models to
              inform the policy
              with
              rich context from the data. Experimentally, we demonstrate that AutoMA is scalable across a broad
              spectrum
              of tasks and significantly outperforms baseline models in six simulated and real-world manipulation
              tasks,
              achieving success rate improvements of up to 56%. Furthermore, we illustrate how AutoMA facilitates
              re-using the modules when transferring the policy to different robots with only 20% of the original data
              scale, significantly improving data efficiency.
            </p>
            <!-- <p>
              We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
              photos/videos into deformable NeRF
              models that allow for photorealistic renderings of the subject from arbitrary
              viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
              using a
              rig with two mobile phones that take time-synchronized photos, yielding train/validation
              images of the same pose at different viewpoints. We show that our method faithfully
              reconstructs non-rigidly deforming scenes and reproduces unseen views with high
              fidelity.
            </p> -->
          </div>
        </div>
      </div>
      <!--/ Abstract. -->



      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/OdBMquRUTdU" frameborder="0" allow="autoplay; encrypted-media"
              allowfullscreen></iframe>
          </div>
        </div>
      </div> -->
      <!--/ Paper video. -->
    </div>
  </section>




  <section class="section">
    <div class="container is-max-desktop">

      <!--/ video for 3 start-->
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Put the coke can in the bin"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_00.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Sort the coke can"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_01.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Put the coke into the bin"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_02.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
      <!--/ video for 3 end-->


      <!--/ video for 3 start-->
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              Testing trial 01 with distractors
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_10.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              Testing trial 02 with distractors
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_11.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              Testing trial 03 with distractors
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_12.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
      <!--/ video for 3 end-->

      <!--/ video for 3 start-->
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              Testing trial 01 with no distractor
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_20.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              Testing trial 02 with no distractor
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_21.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              Testing trial 03 with no distractor
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_22.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
      <!--/ video for 3 end-->

      <!--/ video for 3 start-->
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              Testing trial 01 with distractors
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_30.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              Testing trial 02 with no distractor
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_31.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              Testing trial 03 with no distractor
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_32.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
      <!--/ video for 3 end-->



      <!-- method. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">AutoMA</h2>

          <!-- Interpolating. -->
          <h3 class="title is-4">Overview</h3>
          <div class="content has-text-justified">
            <p>
              This AutoMA framework bridges the gap between modular and end-to-end learning, enabling the reuse of
              functional building blocks. In summary, our contributions are as follows: (1) <strong>Leveraging Rich
                Context</strong>: Our approach leverages rich context by decomposing tasks into hierarchical modules and
              semantically extracting the underlying context for each module; (2) <strong>Improving Data
                Efficiency</strong>:
              AutoMA enhances data efficiency by enabling the reuse of functional building blocks when transferring
              tasks to different embodiments; (3) <strong>Empirical Evaluations</strong>: AutoMA demonstrates
              significant
              improvements, achieving up to 56% success rate improvements in complex manipulation tasks with
              distractors compared to baselines.
            </p>
            <img src="./static/videos/overview_automa.png" class="interpolation-image"
              alt="Interpolate start reference image." />
            <p>
              Diff-Control operates by generating a sequence of actions while incorporating conditioning on previously
              generated actions. In this example, the Diff-Control policy is depicted executing the "Open Lid" task.
              For
              instance, in the second sub-figure,
              the blue trajectory represents previous action trajectory, denoted as
              <strong>a<sub>[W<sub>t</sub>]</sub></strong>, while
              the red trajectory
              displays the newly generated sequence of actions, denoted as
              <strong>a<sub>[W<sub>t-h</sub>]</sub></strong>.
            </p>
          </div>


          <!-- Attention Gain -->
          <h3 class="title is-4">Stateful behavior</h3>
          <div class="content has-text-justified">
            <p>
              We find that proposed Diff-Control policy effectively maintains stateful behavior by conditioning its
              actions on prior actions, resulting in consistent action generation. An illustrative example for this
              behavior is shown below: a policy learning to approximate a cosine function. Given single observation at
              time t, stateless policies encounter difficulties in producing accurate generating the continuation of
              trajectories. Due to ambiguities, diffusion policy tends to learn multiple modes. By contrast,
              Diff-Control integrates temporal conditioning allowing it to generate trajectories by considering past
              states. To this end, the proposed approach leverages recent ControlNet architectures to ensure temporal
              consistency in robot action generation.
            </p>
            <center><img src="./static/videos/sine.png" width="600" height="500"></center>
            <p>
              At a given state, Diff-Control policy can utilize prior trajectories to approximate the desired
              function.
              Diffusion policy learns both modes but fails on generating the correct trajectory cosistently,
              Image-BC/BC-Z fails to generate the correct trajectory.
            </p>
          </div>
          <!--/ Attention Gain -->


          <!-- result -->
          <h3 class="title is-4">Tasks</h3>
          <div class="content has-text-justified">
            <p>
            <ul>
              <li><strong>Language Conditioned kitchen tasks</strong>: This task is designed to resemble several tasks
                in the kitchen scenario. </li>
              <li><strong>Open Lid task</strong>: this task is in the kitchen scene with a high-precision requirement.
              </li>
              <li><strong>Duck Scooping task</strong>: we explore the interaction between the policy and fluid
                dynamics.
              </li>
              <li><strong>Drum Beats (3 hits) task</strong>: this task is specifically designed for robots to learn
                periodic
                motions</li>
            </ul>

            </p>
            <img src="./static/videos/task.png" class="interpolation-image" alt="Interpolate start reference image." />
          </div>
          <!--/ result -->

          <!-- result -->
          <h3 class="title is-4">Evaluations</h3>
          <div class="content has-text-justified">
            <p>
            <ul>
              <li><strong>Image-BC</strong>: This baseline adopts an image-to-action agent
                framework, similar to BC-Z, it is built upon ResNet-18 backbone and employs FiLM for conditioning
                using
                CLIP language features. </li>
              <li><strong>ModAttn</strong>: This method employs a transformer-style neural network and uses a modular
                structure to address each sub-aspects of the task via neural attention, it requires a human expert
                correctly identifies components and subtasks to each task.
              </li>
              <li><strong>BC-Z LSTM</strong>: This baseline represents a stateful policy inspired by the BC-Z
                architecture. The incorporation of a prior input is achieved by fusing the prior actions and language
                conditions using MLP and LSTM layers.
              </li>
              <li><strong>Diffusion Policy</strong>: This baseline is a standard diffusion policy.</li>
            </ul>

            </p>
            <center><img src="./static/videos/Table.png" width="800" height="600" class="interpolation-image"
                alt="Interpolate start reference image." /></center>
          </div>
          <!--/ result -->

        </div>
      </div>





      <!-- Concurrent Work. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Links</h2>

          <div class="content has-text-justified">
            <p>
              To assess the performance and effectiveness of our approach, we conducted comparative evaluations
              against
              robot learning policy baselines.
            </p>
            <p>
              <a href="https://diffusion-policy.cs.columbia.edu/">Diffusion Policy
              </a> serves as the base policy in proposed framework, similar design decisions has been made referring
              to
              the visual encodings, hyper-parameters, etc.
            </p>
            <p>
              We adapt <a href="https://github.com/lllyasviel/ControlNet">ControlNet</a> sturcture, expecially the
              zero
              convolusion layers to create Diff-Control in robot trajectory domain.
            </p>
            <p>
              Besides diffusion policy, we compared <a href="https://arxiv.org/abs/2202.02005">Image-BC/BC-Z</a>
              baseline, the <a href="https://arxiv.org/pdf/2212.04573.pdf">ModAttn</a> baseline, and BC-Z LSTM.
            </p>
          </div>
        </div>
      </div>
      <!--/ Concurrent Work. -->

    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{liudiff,
        title={Diff-Control: A Stateful Diffusion-based Policy for Imitation Learning},
        author={Liu, Xiao and Zhou, Yifan and Weigend, Fabian and Sonawani, Shubham and Ikemoto, Shuhei and Amor, Heni Ben}
      }</code></pre>
      <pre><code>@article{liu2024enabling,
        title={Enabling Stateful Behaviors for Diffusion-based Policy Learning},
        author={Liu, Xiao and Weigend, Fabian and Zhou, Yifan and Amor, Heni Ben},
        journal={arXiv preprint arXiv:2404.12539},
        year={2024}
      }</code></pre>
    </div>



  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/Diff-Control.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/liuxiao1468" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a
                href="https://github.com/Diff-Control/Diff-Control.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>