<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="AutoMA: Automated Modular Attention enables Context-Rich Imitation Learning using Foundation Models">
  <meta name="keywords" content="Imitation Learning, Modular Policy">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AutoMA</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/irl_lab.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://auto-ma.github.io/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://languageforrobots.com/">
              Modularity through Attention (ModAttn)
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AutoMA: Automated Modular Attention enables Context-Rich Imitation
              Learning using Foundation Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="http://yifanzhou.com/">Yifan Zhou</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.xiao-liu.me/">Xiao Liu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://quanvuong.github.io/">Quan Vuong</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://henibenamor.weebly.com/">Heni Ben Amor</a><sup>1</sup>,
              </span>
              <!-- <span class="author-block">
                <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
              </span> -->
              <!-- <span class="author-block">
                <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
              </span> -->
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup><a
                  href="https://interactive-robotics.engineering.asu.edu/">Interactive Robotics Lab, Arizona State
                  University</span></a>
              <span class="author-block"><sup>2</sup><a href="https://physicalintelligence.company/">Physical
                  Intelligence</span></a>
            </div>

            <!-- <div class="column has-text-centered">
              <div class="publication-links"> -->
            <!-- PDF Link. -->
            <!-- <span class="link-block">
                  <a href="./static/videos/Diff-Control.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
            <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2011.12948" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
            <!-- Video Link. -->
            <!-- <span class="link-block">
                  <a href="./static/videos/task_sequence.mp4" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
            <!-- Code Link. -->
            <!-- <span class="link-block">
                  <a href="https://github.com/ir-lab/Diff-Control"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
            <!-- Dataset Link. -->
            <!-- <span class="link-block">
                  <a href="https://www.dropbox.com/scl/fo/2onj92s7gewettu1rjg3d/h?rlkey=1d4dnhwf3z6s4a51lopgocby5&dl=0"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div> -->

            <!-- </div> -->
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">

        <img src="./static/videos/teaser-1.png" class="interpolation-image" alt="Interpolate start reference image." />

        <!-- <video id="teaser" autoplay controls muted loop playsinline height="100%">
          <source src="./static/videos/task_sequence.mp4" type="video/mp4">
        </video> -->
        <h2 class="subtitle has-text-centered">

          Given the task "<i>stack the blue block on the red block</i> ", LLMs design hierarchical modules h by
          decomposing the task into several sub-task modules, while VLMs supervises the module. The policy network,
          enriched with this context, can then effectively execute the task.

        </h2>
      </div>
    </div>
  </section>


  <!-- <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/steve.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/chair-tp.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/shiba.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fullbody.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/blueshirt.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/mask.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-coffee">
            <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/coffee.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/toby2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Although imitation learning offers an appealing
              framework for policy learning, scaling it is challenging due
              to the lack of high-quality expert data. Previous works have
              addressed this by modifying model design and increasing state
              diversity to improve data quality, but these approaches often
              overlook the rich contextual information inherent in the data
              itself. In this paper, we propose an automated learning framework
               that leverages large language models (LLMs) and vision-language 
              models (VLMs) to provide context-rich supervision
              for Modular Attention based imitation learning policies, denoted
              as <strong>AutoMA</strong>. Specifically, LLMs are used for hierarchical
              modular design, and VLMs provide supervision signals for the
              different modules. <strong>AutoMA</strong> thus leverages the capabilities of
              foundational models to inform the policy with rich context
              from the data. Experimentally, we demonstrate that AutoMA
              is scalable across a broad spectrum of tasks and significantly
              outperforms baseline models in six simulated and real-world
              manipulation tasks, achieving success rate improvements of up
              to 63.3%. Furthermore, we illustrate how <strong>AutoMA</strong> facilitates
              re-using the modules when transferring the policy to different
              robots with only 20% of the original data scale, significantly
              improving data efficiency.
            </p>
            <!-- <p>
              We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
              photos/videos into deformable NeRF
              models that allow for photorealistic renderings of the subject from arbitrary
              viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
              using a
              rig with two mobile phones that take time-synchronized photos, yielding train/validation
              images of the same pose at different viewpoints. We show that our method faithfully
              reconstructs non-rigidly deforming scenes and reproduces unseen views with high
              fidelity.
            </p> -->
          </div>
        </div>
      </div>
      <!--/ Abstract. -->



      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/OdBMquRUTdU" frameborder="0" allow="autoplay; encrypted-media"
              allowfullscreen></iframe>
          </div>
        </div>
      </div> -->
      <!--/ Paper video. -->
    </div>
  </section>






  <section class="section">
    <div class="container is-max-desktop content">





      <!-- method. -->
      <!-- <div class="columns is-centered has-text-centered"> -->
        <!-- <div class="columns has-text-centered is-full-width"> -->
        <!-- <div class="column is-full-width"> -->
          <div>
          <div>
          <h2 class="title is-3"></h2>

          <!-- Interpolating. -->
          <h3 class="title is-4">Overview</h3>
          <div class="content has-text-justified">
            <p>
              <strong>AutoMA</strong> focuses on training a language-conditioned robot policy. 
              It operates in two key steps: (1)
              <strong>Automatic Hierarchy and Module Design</strong>. In this phase,
              we employ LLMs and VLMs for task comprehension, which
              autonomously generates a hierarchy of modules relevant
              to the task, along with the corresponding training labels.
              (2) <strong>End-to-End Training of Modular Attention</strong>. Once
              the training data and module design are established, the
              second step involves training a transformer model end-to-end,
              embedding the modular hierarchy to guide the learning
              process effectively.
            </p>
            <img src="./static/videos/overview_automa.png" class="interpolation-image"
              alt="Interpolate start reference image." />
            <p>
              LLMs decompose high-level tasks, such as "<i>stack the red block on the brown block</i>" into modules
              and construct a hierarchy among them. <strong>AutoMA</strong> integrates this hierarchy using slot
              attention mechanisms for
              policy learning. For instance, the <i>Task</i> and <i>Object</i> only focus on <i>Lang</i> tokens in
              the first attention layer, the <i>ObjLoc</i> module attends to <i>Image</i> to localizing objects, and
              the <i>CTRL</i> module attends to <i>EE</i>, <i>task</i>, and <i>ObjLoc</i> to generate final
              robot actions.
            </p>
          </div>


          <!-- Attention Gain -->
          <h3 class="title is-4">Method Step 1: Automatic Synthesis of Hierarchy and Modules</h3>
          <div class="content has-text-justified">
            <p>
              For instance, in a stacking task, the high-level task is decomposed into several
              sub-tasks: (1) understanding the required action type, (2) identifying which object to manipulate, (3)
              locating the object in the image, and (4) generating the end-effector controls. As shown in the figure,
              these sub-tasks form a hierarchical structure where <i>Lang</i>, <i>Image</i>, and <i>EE</i> are considered as input modules, <i>Task</i>,
              <i>Object</i>, <i>ObjLoc</i>, and <i>CTRL</i> are sub-task modules.
            </p>
            <center><img src="./static/videos/h.png" width="400" height="300"></center>
            <p>
              
            </p>
              













              <p>
                Large Language Models (LLMs) are employed to automate the generation of task hierarchies. The goal is to analyze an expert demonstration and construct an appropriate hierarchy. As illustrated in the code below, the prompt template used in our approach consists of three main components: (1) A preamble detailing the task of hierarchy generation; (2) An in-context learning example that demonstrates the input-output format; and (3) The actual input derived from the expert demonstrations.
              </p>

              <div style="height: 300px; width: 100%; overflow:auto; word-wrap: break-word">
                <pre style="word-wrap: break-word; width: 100%"><code style="word-wrap: break-word; width: 100%">

                  <strong>**Task**</strong>
                  You will be presented with a short video of a robot performing a task, together with a sentence describing the task. 
                  Your task is to create an understanding of the tasks, with the goal of performing imitation learning.
                  The understandings will include the following:
                  1) According to the video and the sentence, what information should the robot pay attention to?
                  2) Regarding the selected information, how do they form up a hierarchical structure?
                  
                  <strong>**Example**</strong>
                  All_modalities_available: 
                  - I1: Natural Language Instruction
                  - I2: Single Image of Current Time
                  - I3: Joint State of Robot
                  - I4: End-effector location of Robot
                  All_output_ground_truths:
                  - G1: Action Trajectory
                  Requirements:
                  - No modules should have the same (input ground truth, output ground truth) pair.
                  APIs available:
                  - get_from_language(target)
                  - get_from_image(target)
                  Sentence: 
                  - Lift up the cube.
                  Video:
                  - &lt;frame1&gt;&lt;frame2&gt;&lt;frame3&gt;&lt;frame4&gt;&lt;frame5&gt;
                  
                  Understanding:
                  R1 = Query(key=I1); N1 = Decode(R1); # Get the task to perform, from language. Now R1 contains the task to perform.
                  R2 = Query(key=I1); N2 = Decode(R2); # Get the object to manipulate, from language. Now R2 contains the object to manipulate.
                  R3 = Query(key=I2); N3 = Decode(R3); # Get the location of the object, from object to manipulate and image. Now R3 contains the location of the object.
                  R4 = Query(key=I3); N4 = Decode(R4); # Get the location of the robot, from joint state. Now R4 contains the location of the robot.
                  R5 = Query(key=[R1, R3, R4]); N5 = Decode(R5); # Get the plan to perform the task, from task, object location, and robot location. Now R5 contains the plan to perform the task.
                  
                  Generating labels:
                  - N1: get_from_language("what task to perform")
                  - N2: get_from_language("what object to manipulate")
                  - N3: get_from_image(N2)
                  - N4: I4
                  - N5: G1
                  
                  <strong>**Your Response**</strong>
                  All_modalities_available: 
                  - &lt;Modality 1&gt;&lt;Modality 2&gt;, …, &lt;Modality N&gt;
                  All_output_ground_truths:
                  - &lt;Output 1&gt;
                  Requirements:
                  - No modules should have the same (input ground truth, output ground truth) pair.
                  Sentence: 
                  - &lt;sentence&gt;
                  Video:
                  - &lt;frame1&gt;&lt;frame2&gt;&lt;frame3&gt;&lt;frame4&gt;&lt;frame5&gt;
                  
                  Understanding:                  
              </code></pre></div>

          </div>
          <!--/ Attention Gain -->



          <h3 class="title is-4">Method Step 2: End-to-End Training of Modular Attention</h3>
          <div class="content has-text-justified">
            <p>
              The proposed modules address specific sub-tasks, such as detecting the robot's 
              end-effector or calculating the distance between the robot and a target object. 
              To achieve this, we leverage modern attention mechanisms to manage information 
              flow within attention layers, ensuring the network focuses 
              on critical inputs.
            </p>
            <p>
              Specifically, we implement a supervised attention mechanism that allows for customized 
              information routing (defined by the automatically generatedhierarchy), 
              facilitating the creation of modular structures within an end-to-end 
              neural network. The core concept behind this mechanism is that the hierarchy reveals 
              knowledge about optimal token pairings. In other words, if the hierarchy points out key 
              tokens that are essential for the queries, their similarity scores can be maximized to 
              improve the network's focus and accuracy.
            </p>
            <center><img src="./static/videos/Layer_example.png" width="200px"></center>
            <p>
              The figure illustrates the attention mechanism which facilitates the module <i>ObjLoc</i>. 
              Queries (Q) are used to identify the most relevant keys (K), 
              producing scores (the gray and yellow matrix) that reflect their alignment.
              The module <i>ObjLoc</i> is designed to take in the object information, and locate the object in the image.
              As shown in the figure, there are 2 object in the query. 
              For each object query, the most relevant image patch (light blue) is identified, 
              and the corresponding value, which is considered to contain the image  is used as the output.
              In order to do so, the corresponding score in the attention map should be maximized during optimization.
              This process is called <strong><i>supervised attention</i></strong>. 
              In this way, we define our first training objective, the Task Hierarchy Loss:
            </p>
            <center><img src="./static/videos/task_hierarchy_loss.png" width="400px"></center>
            <p>
              The task hierarchy loss <i>L<sub>h</sub></i> optimizes towards 
              fetching the key <i><strong>k</strong><sub>n</sub></i>
               for the 
              given module’s query <i><strong>q</strong><sub>n</sub></i>. Therefore, it
              can guide the attention of the sub-modules to enforce the
              controlled data flow.
            </p>
            <p>
              Additionally, we also need to supervise the actual outputs of each module. To do so, we 
              create MLPs for each module, which serve as prediction heads. The attention output token
              on of the n-th module is passed through MLP<sub>n</sub>, which is
              supervised through the sub-task label <i>l</i><sub>n</sub>. 
              We define the second training objective, the Sub-Task Loss as following:
            </p>
            <center><img src="./static/videos/sub_task_loss.png" width="400px"></center>
            <p>The overall training objective is <i>L<sub>h</sub></i> + <i>L<sub>sub</sub></i>. 
              The result of this training process
              is a robot policy that generates actions, which is instantiated
              by a transformer network and is embedded by modules that
              correspond for the sub-tasks.</p>
          </div>



          <!-- result -->
          <h3 class="title is-4">Experiment Tasks</h3>
          <div class="content has-text-justified">
            <p>
              we evaluate the effectiveness of AutoMA across multiple manipulation tasks, each with distinct setups:
            <ul>
              <li><strong>Stacking</strong>: task (a)-(c) for block stacking tasks </li>
              <li><strong>Sorting</strong>: task (d)-(e) for object sorting tasks with and without distractors
              <li><strong>TableTop</strong>: task (f) for manipulation in real-world with distractors
              </li>
            </ul>

            </p>
            <img src="./static/videos/automa_task.png" class="interpolation-image"
              alt="Interpolate start reference image." />
          </div>
          <!--/ result -->

          <!-- result -->
          <h3 class="title is-4">Evaluations</h3>
          <div class="content has-text-justified">
            <p>
            <ul>
              <li><strong>Image-BC</strong>: This baseline adopts an image-to-action agent
                framework, similar to BC-Z, it is built upon ResNet-18 backbone and employs FiLM for conditioning
                using
                CLIP language features. </li>
              <li><strong>Diffusion Policy</strong>: This baseline is a standard diffusion-based policy, We adopt the 1D
                temporal convolutional networks and construct the U-net backbone.
              </li>
              <li><strong>ModAttn</strong>: This baseline is a transformer based network, which shares the same
                architecture with our proposed AutoMA, but trained end-to-end without enforcing rich contexts.
              </li>
            </ul>

            </p>
            <center><img src="./static/videos/Table_automa.png" width="600" height="400" class="interpolation-image"
                alt="Interpolate start reference image." /></center>
          </div>
          <!--/ result -->

        </div>
      </div>





      <!-- Concurrent Work. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Links</h2>

          <div class="content has-text-justified">
            <p>
              To assess the performance and effectiveness of our approach, we conducted comparative evaluations
              against
              robot learning policy baselines.
            </p>
            <p>
              <a href="https://diffusion-policy.cs.columbia.edu/">Diffusion Policy
              </a> serves as the base policy in proposed framework, similar design decisions has been made referring
              to
              the visual encodings, hyper-parameters, etc.
            </p>
            <p>
              We adapt <a href="https://github.com/lllyasviel/ControlNet">ControlNet</a> sturcture, expecially the
              zero
              convolusion layers to create Diff-Control in robot trajectory domain.
            </p>
            <p>
              Besides diffusion policy, we compared <a href="https://arxiv.org/abs/2202.02005">Image-BC/BC-Z</a>
              baseline, the <a href="https://arxiv.org/pdf/2212.04573.pdf">ModAttn</a> baseline, and BC-Z LSTM.
            </p>
          </div>
        </div>
      </div> -->
      <!--/ Concurrent Work. -->


      <!--/ video for 3 start-->
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Put the coke can in the bin"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_00.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Sort the coke can"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_01.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Put the coke into the bin"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_02.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
      <!--/ video for 3 end-->


      <!--/ video for 3 start-->
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Put the bread in the bin"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_10.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Sort the coke can"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_11.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Pick the milk and put it in the bin"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_12.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
      <!--/ video for 3 end-->

      <!--/ video for 3 start-->
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              "Stack the red block on the green block"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_20.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              "Put the red block on the green block"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_21.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              "Place red block over green block"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_22.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
      <!--/ video for 3 end-->

      <!--/ video for 3 start-->
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              "Put the red block on the green block"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_30.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              "Stack blue blcok on the green block"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_31.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              "Place the brown block on the red block"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_32.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
      <!--/ video for 3 end-->

    </div>
  </section>

<!-- 
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Model Details</h2>
      <p>
        AutoMA embeds modular designs through an attention mechanism, which operates using three components: queries
        (Q), keys (K), and values (V). The query (Q) identifies the most relevant keys (K), producing scores that
        reflect their alignment. These scores are normalized and used to weight the corresponding values (V), thereby
        aggregating the most relevant information.
      </p>
    </div>

  </section> -->

<!-- 
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Automated Task Hierarchy Generation</h2>
      <p>
        Large Language Models (LLMs) are employed to automate the generation of task hierarchies. The goal is to analyze an expert demonstration and construct an appropriate hierarchy. As illustrated in the code below, the prompt template used in our approach consists of three main components: (1) A preamble detailing the task of hierarchy generation; (2) An in-context learning example that demonstrates the input-output format; and (3) The actual input derived from the expert demonstrations.
      </p>
      <div style="height: 300px; overflow:auto; word-wrap: break-word">
      <pre style="word-wrap: break-word"><code>
        <strong>**Task**</strong>
        You will be presented with a short video of a robot performing a task, together with a sentence describing the task. 
        Your task is to create an understanding of the tasks, with the goal of performing imitation learning.
        The understandings will include the following:
        1) According to the video and the sentence, what information should the robot pay attention to?
        2) Regarding the selected information, how do they form up a hierarchical structure?
        
        <strong>**Example**</strong>
        All_modalities_available: 
        - I1: Natural Language Instruction
        - I2: Single Image of Current Time
        - I3: Joint State of Robot
        - I4: End-effector location of Robot
        All_output_ground_truths:
        - G1: Action Trajectory
        Requirements:
        - No modules should have the same (input ground truth, output ground truth) pair.
        APIs available:
        - get_from_language(target)
        - get_from_image(target)
        Sentence: 
        - Lift up the cube.
        Video:
        - &lt;frame1&gt;&lt;frame2&gt;&lt;frame3&gt;&lt;frame4&gt;&lt;frame5&gt;
        
        Understanding:
        R1 = Query(key=I1); N1 = Decode(R1); # Get the task to perform, from language. Now R1 contains the task to perform.
        R2 = Query(key=I1); N2 = Decode(R2); # Get the object to manipulate, from language. Now R2 contains the object to manipulate.
        R3 = Query(key=I2); N3 = Decode(R3); # Get the location of the object, from object to manipulate and image. Now R3 contains the location of the object.
        R4 = Query(key=I3); N4 = Decode(R4); # Get the location of the robot, from joint state. Now R4 contains the location of the robot.
        R5 = Query(key=[R1, R3, R4]); N5 = Decode(R5); # Get the plan to perform the task, from task, object location, and robot location. Now R5 contains the plan to perform the task.
        
        Generating labels:
        - N1: get_from_language("what task to perform")
        - N2: get_from_language("what object to manipulate")
        - N3: get_from_image(N2)
        - N4: I4
        - N5: G1
        
        <strong>**Your Response**</strong>
        All_modalities_available: 
        - &lt;Modality 1&gt;&lt;Modality 2&gt;, …, &lt;Modality N&gt;
        All_output_ground_truths:
        - &lt;Output 1&gt;
        Requirements:
        - No modules should have the same (input ground truth, output ground truth) pair.
        Sentence: 
        - &lt;sentence&gt;
        Video:
        - &lt;frame1&gt;&lt;frame2&gt;&lt;frame3&gt;&lt;frame4&gt;&lt;frame5&gt;
        
        Understanding:
        
    </code></pre></div>
    </div>

  </section> -->


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/Diff-Control.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/liuxiao1468" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a
                href="https://github.com/Diff-Control/Diff-Control.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>