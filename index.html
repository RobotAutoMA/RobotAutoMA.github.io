<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="AutoMA: Automated Modular Attention enables Context-Rich Imitation Learning using Foundation Models">
  <meta name="keywords" content="Imitation Learning, Modular Policy">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AutoMA</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/irl_lab.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://auto-ma.github.io/">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://languageforrobots.com/">
              Modularity through Attention (ModAttn)
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">AutoMA: Automated Modular Attention enables Context-Rich Imitation
              Learning using Foundation Models</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="http://yifanzhou.com/">Yifan Zhou</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.xiao-liu.me/">Xiao Liu</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://quanvuong.github.io/">Quan Vuong</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://henibenamor.weebly.com/">Heni Ben Amor</a><sup>1</sup>,
              </span>
              <!-- <span class="author-block">
                <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
              </span> -->
              <!-- <span class="author-block">
                <a href="https://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
              </span> -->
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup><a
                  href="https://interactive-robotics.engineering.asu.edu/">Interactive Robotics Lab, Arizona State
                  University</span></a>
              <span class="author-block"><sup>2</sup><a href="https://physicalintelligence.company/">Physical
                  Intelligence</span></a>
            </div>

            <!-- <div class="column has-text-centered">
              <div class="publication-links"> -->
            <!-- PDF Link. -->
            <!-- <span class="link-block">
                  <a href="./static/videos/Diff-Control.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
            <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2011.12948" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span> -->
            <!-- Video Link. -->
            <!-- <span class="link-block">
                  <a href="./static/videos/task_sequence.mp4" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span> -->
            <!-- Code Link. -->
            <!-- <span class="link-block">
                  <a href="https://github.com/ir-lab/Diff-Control"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->
            <!-- Dataset Link. -->
            <!-- <span class="link-block">
                  <a href="https://www.dropbox.com/scl/fo/2onj92s7gewettu1rjg3d/h?rlkey=1d4dnhwf3z6s4a51lopgocby5&dl=0"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div> -->

            <!-- </div> -->
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">

        <img src="./static/videos/teaser-1.png" class="interpolation-image" alt="Interpolate start reference image." />

        <!-- <video id="teaser" autoplay controls muted loop playsinline height="100%">
          <source src="./static/videos/task_sequence.mp4" type="video/mp4">
        </video> -->
        <h2 class="subtitle has-text-centered">

          Given the task "<i>stack the blue block on the red block</i> ", LLMs design hierarchical modules h by
          decomposing the task into several sub-task modules, while VLMs supervises the module. The policy network,
          enriched with this context, can then effectively execute the task.

        </h2>
      </div>
    </div>
  </section>


  <!-- <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-steve">
            <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/steve.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-chair-tp">
            <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/chair-tp.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-shiba">
            <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/shiba.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-fullbody">
            <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fullbody.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-blueshirt">
            <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/blueshirt.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-mask">
            <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/mask.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-coffee">
            <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/coffee.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-toby">
            <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/toby2.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section> -->


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Although imitation learning offers an appealing framework for policy learning, scaling it is challenging
              due to the lack of high-quality expert data. Previous works have addressed this by modifying model
              design
              and increasing state diversity to improve data quality, but these approaches often overlook the rich
              contextual information inherent in the data itself. In this paper, we propose an automated learning
              framework that leverages large language models (LLMs) and vision-language models (VLMs) to provide
              context-rich supervision for Modular Attention based imitation learning policies, denoted as
              <strong>AutoMA</strong>.
              Specifically, LLMs are used for hierarchical modular design, and VLMs provide supervision signals for
              the
              different modules. <strong>AutoMA</strong> thus leverages the capabilities of foundational models to
              inform the policy
              with
              rich context from the data. Experimentally, we demonstrate that AutoMA is scalable across a broad
              spectrum
              of tasks and significantly outperforms baseline models in six simulated and real-world manipulation
              tasks,
              achieving success rate improvements of up to 56%. Furthermore, we illustrate how AutoMA facilitates
              re-using the modules when transferring the policy to different robots with only 20% of the original data
              scale, significantly improving data efficiency.
            </p>
            <!-- <p>
              We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
              photos/videos into deformable NeRF
              models that allow for photorealistic renderings of the subject from arbitrary
              viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
              using a
              rig with two mobile phones that take time-synchronized photos, yielding train/validation
              images of the same pose at different viewpoints. We show that our method faithfully
              reconstructs non-rigidly deforming scenes and reproduces unseen views with high
              fidelity.
            </p> -->
          </div>
        </div>
      </div>
      <!--/ Abstract. -->



      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/OdBMquRUTdU" frameborder="0" allow="autoplay; encrypted-media"
              allowfullscreen></iframe>
          </div>
        </div>
      </div> -->
      <!--/ Paper video. -->
    </div>
  </section>




  <section class="section">
    <div class="container is-max-desktop">





      <!-- method. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">AutoMA</h2>

          <!-- Interpolating. -->
          <h3 class="title is-4">Overview</h3>
          <div class="content has-text-justified">
            <p>
              This AutoMA framework bridges the gap between modular and end-to-end learning, enabling the reuse of
              functional building blocks. In summary, our contributions are as follows: (1) <strong>Leveraging Rich
                Context</strong>: Our approach leverages rich context by decomposing tasks into hierarchical modules and
              semantically extracting the underlying context for each module; (2) <strong>Improving Data
                Efficiency</strong>:
              AutoMA enhances data efficiency by enabling the reuse of functional building blocks when transferring
              tasks to different embodiments; (3) <strong>Empirical Evaluations</strong>: AutoMA demonstrates
              significant
              improvements, achieving up to 56% success rate improvements in complex manipulation tasks with
              distractors compared to baselines.
            </p>
            <img src="./static/videos/overview_automa.png" class="interpolation-image"
              alt="Interpolate start reference image." />
            <p>
              LLMs decompose high-level tasks, such as "<i>stack the red block on the brown block</i>" into modules
              and construct a hierarchy among them. <strong>AutoMA</strong> integrates this hierarchy using slot
              attention mechanisms for
              policy learning. For instance, the <i>Task</i> and <i>Object</i> only focus on <i>Lang</i> tokens in
              the first attention layer, the <i>ObjLoc</i> module attends to <i>Image</i> to localizing objects, and
              the <i>CTRL</i> module attends to <i>EE</i>, <i>task</i>, and <i>ObjLoc</i> to generate final
              robot actions.
            </p>
          </div>


          <!-- Attention Gain -->
          <h3 class="title is-4">Modularity</h3>
          <div class="content has-text-justified">
            <p>
              For instance, in a stacking task, the high-level task is decomposed into several
              sub-tasks: (1) understanding the required action type, (2) identifying which object to manipulate, (3)
              locating the object in the image, and (4) generating the end-effector controls. As shown in the figure,
              these sub-tasks form a hierarchical structure
            </p>
            <center><img src="./static/videos/h.png" width="400" height="300"></center>
            <p>
              where <i>Lang</i>, <i>Image</i>, and <i>EE</i> are considered as input modules, <i>Task</i>,
              <i>Object</i>, <i>ObjLoc</i>, and <i>CTRL</i> are sub-task modules.
            </p>
          </div>
          <!--/ Attention Gain -->


          <!-- result -->
          <h3 class="title is-4">Tasks</h3>
          <div class="content has-text-justified">
            <p>
              we evaluate the effectiveness of AutoMA across multiple manipulation tasks, each with distinct setups:
            <ul>
              <li><strong>Stacking</strong>: task (a)-(c) for block stacking tasks </li>
              <li><strong>Sorting</strong>: task (d)-(e) for object sorting tasks with and without distractors
              <li><strong>TableTop</strong>: task (f) for manipulation in real-world with distractors
              </li>
            </ul>

            </p>
            <img src="./static/videos/automa_task.png" class="interpolation-image"
              alt="Interpolate start reference image." />
          </div>
          <!--/ result -->

          <!-- result -->
          <h3 class="title is-4">Evaluations</h3>
          <div class="content has-text-justified">
            <p>
            <ul>
              <li><strong>Image-BC</strong>: This baseline adopts an image-to-action agent
                framework, similar to BC-Z, it is built upon ResNet-18 backbone and employs FiLM for conditioning
                using
                CLIP language features. </li>
              <li><strong>Diffusion Policy</strong>: This baseline is a standard diffusion-based policy, We adopt the 1D
                temporal convolutional networks and construct the U-net backbone.
              </li>
              <li><strong>ModAttn</strong>: This baseline is a transformer based network, which shares the same
                architecture with our proposed AutoMA, but trained end-to-end without enforcing rich contexts.
              </li>
            </ul>

            </p>
            <center><img src="./static/videos/Table_automa.png" width="600" height="400" class="interpolation-image"
                alt="Interpolate start reference image." /></center>
          </div>
          <!--/ result -->

        </div>
      </div>





      <!-- Concurrent Work. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Related Links</h2>

          <div class="content has-text-justified">
            <p>
              To assess the performance and effectiveness of our approach, we conducted comparative evaluations
              against
              robot learning policy baselines.
            </p>
            <p>
              <a href="https://diffusion-policy.cs.columbia.edu/">Diffusion Policy
              </a> serves as the base policy in proposed framework, similar design decisions has been made referring
              to
              the visual encodings, hyper-parameters, etc.
            </p>
            <p>
              We adapt <a href="https://github.com/lllyasviel/ControlNet">ControlNet</a> sturcture, expecially the
              zero
              convolusion layers to create Diff-Control in robot trajectory domain.
            </p>
            <p>
              Besides diffusion policy, we compared <a href="https://arxiv.org/abs/2202.02005">Image-BC/BC-Z</a>
              baseline, the <a href="https://arxiv.org/pdf/2212.04573.pdf">ModAttn</a> baseline, and BC-Z LSTM.
            </p>
          </div>
        </div>
      </div> -->
      <!--/ Concurrent Work. -->


      <!--/ video for 3 start-->
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Put the coke can in the bin"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_00.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Sort the coke can"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_01.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Put the coke into the bin"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_02.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
      <!--/ video for 3 end-->


      <!--/ video for 3 start-->
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Put the bread in the bin"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_10.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Sort the coke can"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_11.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Sort Object</h2>
            <p>
              "Pick the milk and put it in the bin"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_12.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
      <!--/ video for 3 end-->

      <!--/ video for 3 start-->
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              "Stack the red block on the green block"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_20.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              "Put the red block on the green block"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_21.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              "Place red block over green block"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_22.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
      <!--/ video for 3 end-->

      <!--/ video for 3 start-->
      <div class="columns is-centered has-text-centered">

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              "Put the red block on the green block"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_30.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              "Stack blue blcok on the green block"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_31.mp4" type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <div class="content">
            <h2 class="title is-5">Stack Blocks</h2>
            <p>
              "Place the brown block on the red block"
            </p>
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/video_32.mp4" type="video/mp4">
            </video>
          </div>
        </div>

      </div>
      <!--/ video for 3 end-->

    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Model Details</h2>
      <p>
        AutoMA embeds modular designs through an attention mechanism, which operates using three components: queries
        (Q), keys (K), and values (V). The query (Q) identifies the most relevant keys (K), producing scores that
        reflect their alignment. These scores are normalized and used to weight the corresponding values (V), thereby
        aggregating the most relevant information.
      </p>
    </div>

  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Prompt Templete</h2>
      <p>
        the task for the robot
      </p>
      <pre><code>
        **Task:**
        You are a robot reflection model and your ask is to look at an image and
        look at two state description of the scene and return the more accurate state.
        For example, your command is given as 'stack the 1st block
        on the 2nd block and stack the 3rd block on the 1st block.'.
        Your input is an observation image of the scene, the two state description of the scene,
        the output should be an state which describes more correctly of the scene.
    </code></pre>
      <p>
        the example
      </p>
      <pre><code>
        **Example Responses (you should follow the same JSON output format):**
        Example 1.
        "image_1"
        {
        "task": "stack the red block on the green block and stack the blue block on the red block.",
        "input_state_1": ["red block on the green block", "green block on the table", "blue block on the table", "brown block on the table"],
        "input_state_2": ["red block on the table", "green block on the table", "blue block on the table", "brown block on the table"],
        "output_state": ["red block on the table", "blue block on the table", "green block on the table", "brown block on the table"],
        "reason": "In the above image, all for blocks are not yet stacked and they are all on the table, so the input_state_2 is more accurate."
        }
    </code></pre>
    </div>

  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/Diff-Control.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/liuxiao1468" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a
                href="https://github.com/Diff-Control/Diff-Control.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>